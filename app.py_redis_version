#!/usr/bin/env python3
import os
import io
import json
import hashlib
import tempfile
from datetime import datetime
from typing import Optional, Tuple, List

from dotenv import load_dotenv
load_dotenv()

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")

import numpy as np
import streamlit as st
import redis

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaLLM
from sentence_transformers import SentenceTransformer

# -----------------------------
# Simple login with two roles
# -----------------------------
CHATBOT_PASSWORD = os.getenv("CHATBOT_PASSWORD", "chatbot123")   # Chat access only
UPLOAD_PASSWORD = os.getenv("UPLOAD_PASSWORD", "upload123")      # Chat + Upload access

if "authenticated" not in st.session_state:
    st.session_state["authenticated"] = False
if "can_upload" not in st.session_state:
    st.session_state["can_upload"] = False

if not st.session_state["authenticated"]:
    with st.form("login", clear_on_submit=True):
        pw = st.text_input("Password", type="password")
        submit = st.form_submit_button("Login")
        if submit:
            if pw == CHATBOT_PASSWORD:
                st.session_state["authenticated"] = True
                st.session_state["can_upload"] = False
                st.success("âœ… Logged in (chat only)")
            elif pw == UPLOAD_PASSWORD:
                st.session_state["authenticated"] = True
                st.session_state["can_upload"] = True
                st.success("âœ… Logged in (chat + upload)")
            else:
                st.error("âŒ Wrong password")
    st.stop()

# -----------------------------
# Settings
# -----------------------------
INDEX_DIR = "vector_store"               # FAISS index directory (persistent)
UPLOADS_DIR = "uploads"                  # Where we keep original files
MANIFEST_PATH = os.path.join(INDEX_DIR, "manifest.json")  # Track what's ingested

DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "mistral")                # Ollama model
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")  # embedding
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "120"))
TOP_K = int(os.getenv("TOP_K", "4"))

# Redis & caching settings
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_EXACT_TTL = int(os.getenv("REDIS_EXACT_TTL", str(60 * 60 * 24 * 7)))  # 7 days default
SEMANTIC_SIM_THRESHOLD = float(os.getenv("SEMANTIC_SIM_THRESHOLD", "0.92"))
SEMANTIC_MAX_SCAN = int(os.getenv("SEMANTIC_MAX_SCAN", "1000"))  # max semantic items to scan

os.makedirs(UPLOADS_DIR, exist_ok=True)

# -----------------------------
# Helpers
# -----------------------------
def file_sha256(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

def normalize_question(q: str) -> str:
    # normalise whitespace and lowercase for exact-match caching
    return " ".join(q.strip().lower().split())

def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def load_manifest():
    if not os.path.exists(MANIFEST_PATH):
        return {"files": {}}
    with open(MANIFEST_PATH, "r") as f:
        return json.load(f)

def save_manifest(mani):
    os.makedirs(INDEX_DIR, exist_ok=True)
    with open(MANIFEST_PATH, "w") as f:
        json.dump(mani, f, indent=2)

@st.cache_resource(show_spinner=False)
def get_embeddings():
    # cached across reruns -> avoids reloading HF model
    return HuggingFaceEmbeddings(model_name=EMBED_MODEL)

@st.cache_resource(show_spinner=False)
def load_vector_store_cached():
    # cache FAISS handle; if not exists return None
    if os.path.exists(INDEX_DIR):
        try:
            return FAISS.load_local(INDEX_DIR, get_embeddings(), allow_dangerous_deserialization=True)
        except Exception:
            return None
    return None

def persist_vector_store(vs: FAISS):
    vs.save_local(INDEX_DIR)

def load_docs_from_path(path: str):
    # choose loader by extension
    lower = path.lower()
    if lower.endswith(".pdf"):
        loader = PyPDFLoader(path)
    elif lower.endswith(".docx"):
        loader = Docx2txtLoader(path)
    elif lower.endswith(".txt") or lower.endswith(".md"):
        loader = TextLoader(path, encoding="utf-8")
    else:
        raise ValueError("Unsupported file type.")
    return loader.load()

def split_docs(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    return splitter.split_documents(docs)

def ensure_vector_store():
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = load_vector_store_cached()
    return st.session_state.vector_store

def update_vector_store(new_docs):
    vs = ensure_vector_store()
    if vs is None:
        vs = FAISS.from_documents(new_docs, get_embeddings())
        st.session_state.vector_store = vs
    else:
        vs.add_documents(new_docs)
    persist_vector_store(vs)

def ingest_uploaded_file(uploaded_file):
    data = uploaded_file.read()
    digest = file_sha256(data)
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    safe_name = f"{digest}{ext}"
    disk_path = os.path.join(UPLOADS_DIR, safe_name)

    mani = load_manifest()
    if digest in mani["files"]:
        return False, mani["files"][digest]["original_name"]

    with open(disk_path, "wb") as f:
        f.write(data)

    docs = load_docs_from_path(disk_path)
    for d in docs:
        d.metadata = d.metadata or {}
        d.metadata.update({
            "source_file": uploaded_file.name,
            "sha256": digest,
        })
    chunks = split_docs(docs)
    update_vector_store(chunks)

    mani["files"][digest] = {
        "original_name": uploaded_file.name,
        "stored_path": disk_path,
        "ingested_at": datetime.utcnow().isoformat() + "Z",
        "num_chunks": len(chunks)
    }
    save_manifest(mani)
    return True, uploaded_file.name

# -----------------------------
# Redis caching layer
# -----------------------------
def get_redis_client() -> redis.Redis:
    return redis.from_url(REDIS_URL, decode_responses=True)

def exact_cache_get(r: redis.Redis, question: str) -> Optional[str]:
    key = "exact:" + sha256_text(normalize_question(question))
    val = r.get(key)
    return val

def exact_cache_set(r: redis.Redis, question: str, response: str):
    key = "exact:" + sha256_text(normalize_question(question))
    r.set(key, response, ex=REDIS_EXACT_TTL)

def semantic_cache_get(r: redis.Redis, query_emb: List[float]) -> Optional[Tuple[str, float]]:
    """
    Scan semantic cache keys and return (response, similarity) if any entry
    has cosine similarity >= threshold. Scans up to SEMANTIC_MAX_SCAN items.
    """
    ids = r.lrange("semcache:ids", 0, SEMANTIC_MAX_SCAN - 1)
    if not ids:
        return None
    best_score = -1.0
    best_resp = None
    # convert query to numpy once
    qv = np.array(query_emb, dtype=np.float32)
    qnorm = np.linalg.norm(qv)
    for sid in ids:
        key = f"semcache:{sid}"
        try:
            rec = r.hgetall(key)
            if not rec:
                continue
            emb_json = rec.get("emb")
            if not emb_json:
                continue
            emb = np.array(json.loads(emb_json), dtype=np.float32)
            # cosine sim
            embnorm = np.linalg.norm(emb)
            if embnorm == 0 or qnorm == 0:
                continue
            sim = float(np.dot(qv, emb) / (qnorm * embnorm))
            if sim > best_score:
                best_score = sim
                best_resp = rec.get("resp")
                if best_score >= SEMANTIC_SIM_THRESHOLD:
                    # early return if above threshold
                    return best_resp, best_score
        except Exception:
            continue
    if best_score >= SEMANTIC_SIM_THRESHOLD:
        return best_resp, best_score
    return None

def semantic_cache_set(r: redis.Redis, query: str, emb: List[float], response: str):
    # generate an ID
    sid = sha256_text(query + "|" + datetime.utcnow().isoformat())[:16]
    key = f"semcache:{sid}"
    r.hset(key, mapping={
        "query": query,
        "resp": response,
        "emb": json.dumps(list(emb)),
        "created_at": datetime.utcnow().isoformat() + "Z"
    })
    # push id into list head (acts like recent-first)
    r.lpush("semcache:ids", sid)
    # optionally trim to keep list reasonably small (e.g., last 5000)
    max_items = int(os.getenv("SEMANTIC_CACHE_MAX_ITEMS", "5000"))
    r.ltrim("semcache:ids", 0, max_items - 1)

# -----------------------------
# Ollama wrapper and prompt builder
# -----------------------------
def stream_ollama_answer(prompt: str, model_name: str, options=None):
    llm = OllamaLLM(model=model_name, options=options or {})
    text = llm.invoke(prompt)
    for i in range(0, len(text), 20):
        yield text[i:i+20]

def build_prompt(context: str, question: str) -> str:
    return (
        "You are a concise assistant. Use ONLY the provided context. "
        "If the answer cannot be found in context, say you don't know.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {question}\n"
        "Answer with brief, direct sentences and cite the source file names if relevant."
    )

# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="ðŸ“š Team Knowledge Chatbot", layout="wide")
st.title("ðŸ“š Team Knowledge Chatbot")

# Sidebar controls
with st.sidebar:
    st.subheader("Settings")
    model = st.selectbox(
        "Ollama model",
        options=["mistral", "llama3.1:8b-instruct", "phi3", "llama3"],
        index=0,
        help="Smaller models answer faster on CPU. 'mistral' is a good default."
    )
    top_k = st.slider("Top-K chunks", min_value=1, max_value=8, value=TOP_K, step=1)
    show_sources = st.checkbox("Show retrieved sources", value=True)
    st.markdown("---")
    st.markdown("**Ingested files:**")
    mani = load_manifest()
    if mani["files"]:
        for info in mani["files"].values():
            st.write(f"- {info['original_name']}  _(chunks: {info['num_chunks']})_")
    else:
        st.caption("No files ingested yet.")

# Init chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Persisted vector store
ensure_vector_store()

# Redis client
redis_client = get_redis_client()

# -----------------------------
# File Upload (restricted)
# -----------------------------
if st.session_state.get("can_upload", False):
    uploaded_files = st.file_uploader(
        "Upload PDF / DOCX / TXT (you can upload multiple files)",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        with st.spinner("Indexingâ€¦"):
            added = 0
            skipped = 0
            for uf in uploaded_files:
                try:
                    ok, _ = ingest_uploaded_file(uf)
                    if ok:
                        added += 1
                    else:
                        skipped += 1
                except Exception as e:
                    st.error(f"Failed to ingest {uf.name}: {e}")
            if added:
                st.success(f"Indexed {added} file(s).")
            if skipped:
                st.info(f"Skipped {skipped} already-ingested file(s).")
else:
    st.info("ðŸ“‚ File upload restricted. You only have chat access.")

# -----------------------------
# Chat Interface with caching
# -----------------------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

question = st.chat_input("Ask a question about your documentsâ€¦")

if question:
    # 1) Exact-match cache lookup
    exact_hit = exact_cache_get(redis_client, question)
    if exact_hit:
        # return cached answer immediately
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)
        with st.chat_message("assistant"):
            st.markdown(exact_hit)
        st.session_state.messages.append({"role": "assistant", "content": exact_hit})
    else:
        # 2) Semantic-cache lookup
        embeddings = get_embeddings()
        try:
            q_emb = embeddings.embed_query(question)
        except Exception:
            # fallback: if embedding fails, set None to force LLM
            q_emb = None

        sem_hit = None
        if q_emb is not None:
            sem_res = semantic_cache_get(redis_client, q_emb)
            if sem_res:
                resp_text, sim = sem_res
                sem_hit = (resp_text, sim)

        if sem_hit:
            resp_text, sim = sem_hit
            # return semantic cached answer
            st.session_state.messages.append({"role": "user", "content": question})
            with st.chat_message("user"):
                st.markdown(question)
            with st.chat_message("assistant"):
                st.markdown(resp_text + f"\n\nâ€” _(cached, similarity={sim:.3f})_")
            st.session_state.messages.append({"role": "assistant", "content": resp_text})
            # also populate exact cache quickly for future exact matches
            exact_cache_set(redis_client, question, resp_text)
        else:
            # 3) No cache -> run retrieval + LLM
            if st.session_state.vector_store is None:
                st.warning("Please upload and index at least one document first.")
            else:
                st.session_state.messages.append({"role": "user", "content": question})
                with st.chat_message("user"):
                    st.markdown(question)

                retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": top_k})
                results = retriever.get_relevant_documents(question)

                context_parts = []
                shown_sources = []
                for d in results:
                    src = d.metadata.get("source_file", "unknown")
                    snippet = d.page_content.strip().replace("\n", " ")
                    context_parts.append(f"[{src}] {snippet}")
                    if show_sources:
                        shown_sources.append(src)
                context_text = "\n\n".join(context_parts[:top_k])

                prompt = build_prompt(context_text, question)
                with st.chat_message("assistant"):
                    stream_area = st.empty()
                    answer_accum = ""
                    options = {"num_ctx": 1024, "temperature": 0, "num_thread": 4}
                    for chunk in stream_ollama_answer(prompt, model_name=model, options=options):
                        answer_accum += chunk
                        stream_area.markdown(answer_accum)
                    if show_sources and shown_sources:
                        unique_src = sorted(set(shown_sources))
                        stream_area.markdown(answer_accum + "\n\nâ€” _Sources:_ " + ", ".join(unique_src))

                final_text = answer_accum if answer_accum else "I couldn't generate an answer."
                if show_sources and shown_sources:
                    final_text += "\n\nâ€” _Sources:_ " + ", ".join(sorted(set(shown_sources)))
                st.session_state.messages.append({"role": "assistant", "content": final_text})

                # store in exact cache and semantic cache
                try:
                    exact_cache_set(redis_client, question, final_text)
                    if q_emb is None:
                        # compute embedding now if not done earlier
                        q_emb = get_embeddings().embed_query(question)
                    semantic_cache_set(redis_client, question, q_emb, final_text)
                except Exception as e:
                    # caching should not break response; log to Streamlit
                    st.warning(f"Warning: caching failed: {e}")
