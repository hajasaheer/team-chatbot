#!/usr/bin/env python3
import os
import hashlib
from datetime import datetime
from typing import Optional, Tuple, List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# Environment hints
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("OMP_NUM_THREADS", str(max(1, (os.cpu_count() or 1) // 2)))
os.environ.setdefault("OPENBLAS_NUM_THREADS", os.environ["OMP_NUM_THREADS"])

from dotenv import load_dotenv
load_dotenv()

import streamlit as st

# LangChain loaders / splitter
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Ollama
try:
    import ollama  # type: ignore
    OLLAMA_NATIVE = True
except Exception:
    OLLAMA_NATIVE = False

try:
    from langchain_ollama import ChatOllama, OllamaLLM  # type: ignore
except Exception:
    ChatOllama = None
    OllamaLLM = None

# Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http import models as qmodels

# Embeddings
from sentence_transformers import SentenceTransformer

# -----------------------------
# Config / defaults
# -----------------------------
UPLOADS_DIR = "uploads"
os.makedirs(UPLOADS_DIR, exist_ok=True)

QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY") or None
DOC_COLLECTION = os.getenv("DOC_COLLECTION", "documents")
CACHE_COLLECTION = os.getenv("CACHE_COLLECTION", "chat_cache")

DEFAULT_MODEL = os.getenv("DEFAULT_MODEL", "llama3")
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2")

CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1200"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "100"))
TOP_K_DEFAULT = int(os.getenv("TOP_K", "6"))
MIN_SCORE_DEFAULT = float(os.getenv("MIN_SCORE", "0.35"))
CONTEXT_CHAR_CAP_DEFAULT = int(os.getenv("CONTEXT_CHAR_CAP", "4000"))
NUM_PREDICT_DEFAULT = int(os.getenv("NUM_PREDICT", "384"))
SEMANTIC_CACHE_MIN_SCORE = float(os.getenv("SEMANTIC_CACHE_MIN_SCORE", "0.50"))

CHATBOT_PASSWORD = os.getenv("CHATBOT_PASSWORD", "chatbot123")
UPLOAD_PASSWORD = os.getenv("UPLOAD_PASSWORD", "upload123")

# -----------------------------
# Session state init (very early)
# -----------------------------
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False
if "can_upload" not in st.session_state:
    st.session_state.can_upload = False
if "messages" not in st.session_state:
    st.session_state.messages = []
if "admin_settings" not in st.session_state:
    # store admin settings persistently
    st.session_state.admin_settings = {
        "top_k": TOP_K_DEFAULT,
        "min_score": MIN_SCORE_DEFAULT,
        "context_char_cap": CONTEXT_CHAR_CAP_DEFAULT,
        "num_predict": NUM_PREDICT_DEFAULT,
        "model_name": DEFAULT_MODEL
    }

# -----------------------------
# Helpers
# -----------------------------
def file_sha256(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

def sha256_text(text: str) -> str:
    return hashlib.sha256(text.encode("utf-8")).hexdigest()

def load_docs_from_path(path: str):
    lower = path.lower()
    if lower.endswith(".pdf"):
        loader = PyPDFLoader(path)
    elif lower.endswith(".docx"):
        loader = Docx2txtLoader(path)
    elif lower.endswith(".txt") or lower.endswith(".md"):
        loader = TextLoader(path, encoding="utf-8")
    else:
        raise ValueError("Unsupported file type.")
    return loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
def split_docs(docs): return splitter.split_documents(docs)

# -----------------------------
# Load embedding model (blocking) and Qdrant client
# -----------------------------
st.info("Loading embedding modelâ€¦")
EMBED_MODEL = SentenceTransformer(EMBED_MODEL_NAME)
EMBED_DIM = EMBED_MODEL.get_sentence_embedding_dimension()
st.success(f"Embedding model loaded ({EMBED_DIM} dims)")

qclient = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

def _collection_needs_recreate(name: str, want_size: int) -> bool:
    try:
        info = qclient.get_collection(collection_name=name)
        stored = int(info.config.params.vectors.size)  # type: ignore[attr-defined]
        return stored != int(want_size)
    except Exception:
        return True

def ensure_collections():
    # documents
    try:
        if _collection_needs_recreate(DOC_COLLECTION, EMBED_DIM):
            qclient.recreate_collection(
                collection_name=DOC_COLLECTION,
                vectors_config=qmodels.VectorParams(size=EMBED_DIM, distance=qmodels.Distance.COSINE)
            )
    except Exception as e:
        st.error(f"Qdrant init (documents) failed: {e}")
        raise
    # cache
    try:
        if _collection_needs_recreate(CACHE_COLLECTION, EMBED_DIM):
            qclient.recreate_collection(
                collection_name=CACHE_COLLECTION,
                vectors_config=qmodels.VectorParams(size=EMBED_DIM, distance=qmodels.Distance.COSINE)
            )
    except Exception as e:
        st.error(f"Qdrant init (cache) failed: {e}")
        raise

ensure_collections()

def _collection_count(name: str) -> int:
    try:
        return int(qclient.count(collection_name=name, exact=True).count)
    except Exception:
        return 0

# -----------------------------
# Embeddings (multi-threaded)
# -----------------------------
MAX_WORKERS = max(1, min(16, (os.cpu_count() or 1)))
def embed_texts_parallel(texts: List[str], max_workers: int = MAX_WORKERS) -> List[List[float]]:
    embeddings: List[Optional[List[float]]] = [None] * len(texts)
    def worker(i, txt):
        v = EMBED_MODEL.encode(txt, show_progress_bar=False, convert_to_numpy=True)
        return i, v.tolist()
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {ex.submit(worker, i, t): i for i, t in enumerate(texts)}
        for fut in as_completed(futures):
            try:
                i, vec = fut.result()
                embeddings[i] = vec
            except Exception:
                embeddings[futures[fut]] = EMBED_MODEL.encode(texts[futures[fut]]).tolist()
    for idx, e in enumerate(embeddings):
        if e is None:
            embeddings[idx] = EMBED_MODEL.encode(texts[idx]).tolist()
    return embeddings  # type: ignore[return-value]


# =============================
# Qdrant upsert / search
# =============================

def qdrant_upsert_documents(chunks: List, digest: str, source_filename: str):
    texts = [c.page_content for c in chunks]
    metas = [c.metadata for c in chunks]
    ids = [sha256_text(f"{digest}-{i}:{texts[i][:80]}") for i in range(len(texts))]
    vectors = embed_texts_parallel(texts)
    points = []
    for pid, vec, meta, text in zip(ids, vectors, metas, texts):
        payload = meta.copy() if isinstance(meta, dict) else {}
        payload.update({
            "text": text,
            "source_file": source_filename,
            "sha256": digest
        })
        points.append(qmodels.PointStruct(id=pid, vector=vec, payload=payload))
    qclient.upsert(collection_name=DOC_COLLECTION, points=points)


def force_reindex_file(uploaded_file):
    """Delete all chunks for a given file and re-index it"""
    data = uploaded_file.read()
    digest = file_sha256(data)
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    safe_name = f"{digest}{ext}"
    disk_path = os.path.join(UPLOADS_DIR, safe_name)

    # delete old entries
    try:
        qclient.delete(
            collection_name=DOC_COLLECTION,
            points_selector=qmodels.Filter(
                must=[qmodels.FieldCondition(
                    key="source_file", match=qmodels.MatchValue(value=uploaded_file.name)
                )]
            )
        )
    except Exception as e:
        st.warning(f"Could not delete old chunks for {uploaded_file.name}: {e}")

    # overwrite on disk
    with open(disk_path, "wb") as f:
        f.write(data)

    docs = load_docs_from_path(disk_path)
    chunks = split_docs(docs)
    for idx, c in enumerate(chunks):
        c.metadata = c.metadata or {}
        c.metadata.update({
            "source_file": uploaded_file.name,
            "sha256": digest,
            "chunk_index": idx,
            "chunk_size": len(c.page_content)
        })
    qdrant_upsert_documents(chunks, digest, uploaded_file.name)
    return True, uploaded_file.name

# -----------------------------
# Cache helpers
# -----------------------------
def exact_cache_get(query: str) -> Optional[str]:
    key = sha256_text(query)
    try:
        point = qclient.retrieve(collection_name=CACHE_COLLECTION, ids=[key])
        if point and len(point)>0:
            p = point[0]
            if p.payload and "response" in p.payload:
                return p.payload["response"]
    except Exception:
        pass
    return None

def exact_cache_set(query: str, response: str):
    key = sha256_text(query)
    vec = EMBED_MODEL.encode(query, convert_to_numpy=True).tolist()
    payload = {"query": query, "response": response, "ts": datetime.utcnow().isoformat()}
    pt = qmodels.PointStruct(id=key, vector=vec, payload=payload)
    try:
        qclient.upsert(collection_name=CACHE_COLLECTION, points=[pt])
    except Exception:
        pass

def semantic_cache_get(query: str, top_k: int = 1, min_score: float = SEMANTIC_CACHE_MIN_SCORE) -> Optional[Tuple[str,float]]:
    qvec = EMBED_MODEL.encode(query, convert_to_numpy=True).tolist()
    try:
        res = qclient.search(collection_name=CACHE_COLLECTION, query_vector=qvec, limit=top_k, with_payload=True)
        if res:
            item = res[0]
            payload = item.payload or {}
            response_text = payload.get("response")
            score = float(getattr(item, "score", 0.0))
            if response_text and score >= min_score:
                return response_text, score
    except Exception:
        pass
    return None

def semantic_cache_set(query: str, response: str):
    exact_cache_set(query, response)

def clear_cache() -> bool:
    try:
        qclient.delete_collection(collection_name=CACHE_COLLECTION)
    except Exception:
        pass
    qclient.recreate_collection(collection_name=CACHE_COLLECTION, vectors_config=qmodels.VectorParams(size=EMBED_DIM, distance=qmodels.Distance.COSINE))
    return True

# -----------------------------
# Ingest function
# -----------------------------
def ingest_uploaded_file(uploaded_file):
    data = uploaded_file.read()
    digest = file_sha256(data)
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    safe_name = f"{digest}{ext}"
    disk_path = os.path.join(UPLOADS_DIR, safe_name)

    # check Qdrant for sha256
    try:
        scroll_res = qclient.scroll(collection_name=DOC_COLLECTION, scroll_filter=qmodels.Filter(must=[qmodels.FieldCondition(key="sha256", match=qmodels.MatchValue(value=digest))]), limit=1)
        if scroll_res and len(scroll_res)>0:
            return False, uploaded_file.name
    except Exception:
        # ignore and continue
        pass

    # disk save
    with open(disk_path, "wb") as f:
        f.write(data)

    docs = load_docs_from_path(disk_path)
    chunks = split_docs(docs)
    for idx, c in enumerate(chunks):
        c.metadata = c.metadata or {}
        c.metadata.update({"source_file": uploaded_file.name, "sha256": digest, "chunk_index": idx, "chunk_size": len(c.page_content)})
    qdrant_upsert_documents(chunks, digest, uploaded_file.name)
    return True, uploaded_file.name

# -----------------------------
# Ollama streaming wrapper
# -----------------------------
def stream_ollama_answer(prompt: str, model_name: str, options=None):
    options = options or {}
    if OLLAMA_NATIVE:
        try:
            for part in ollama.generate(model=model_name, prompt=prompt, stream=True, options=options):
                chunk = part.get("response","")
                if chunk:
                    yield chunk
            return
        except Exception:
            pass
    if OllamaLLM is not None:
        try:
            llm = OllamaLLM(model=model_name, options=options)
            text = llm.invoke(prompt)
            for i in range(0,len(text),64):
                yield text[i:i+64]
            return
        except Exception:
            pass
    if ChatOllama is not None:
        try:
            llm = ChatOllama(model=model_name, **(options or {}))
            for chunk in llm.stream(prompt):
                yield getattr(chunk,"content",str(chunk))
            return
        except Exception:
            pass
    yield "Ollama backend unavailable."

# -----------------------------
# Prompt builder
# -----------------------------
def build_prompt(context: str, question: str) -> str:
    return (
        "You are a precise assistant that ONLY answers using the provided context.\n"
        "Rules:\n"
        " - If the answer is not in the context, reply exactly: \"I donâ€™t know based on the uploaded documents.\"\n"
        " - Never guess or invent information.\n"
        " - Always cite file names and chunk IDs like [file#chunk].\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {question}\n"
        "Answer:"
    )

# -----------------------------
# UI: single app with Admin and Chat
# -----------------------------
st.set_page_config(page_title="Team Knowledge Chatbot", layout="wide")
st.title("ðŸ“š Team Knowledge Chatbot")

# Simple login form
if not st.session_state.authenticated:
    with st.form("login", clear_on_submit=True):
        pw = st.text_input("Password", type="password")
        submit = st.form_submit_button("Login")
        if submit:
            if pw == CHATBOT_PASSWORD:
                st.session_state.authenticated = True
                st.session_state.can_upload = False
                st.success("Logged in (chat only)")
            elif pw == UPLOAD_PASSWORD:
                st.session_state.authenticated = True
                st.session_state.can_upload = True
                st.success("Logged in (admin)")
            else:
                st.error("Wrong password")
    st.stop()

# Sidebar: simplified chat options + diagnostics
with st.sidebar:
    st.header("Chat")
    current_model = st.session_state.admin_settings.get("model_name", DEFAULT_MODEL)
    st.write("Model:", f"`{current_model}`")
    bypass_cache_ui = st.checkbox("Bypass cache (always query docs)", value=False)
    if st.button("Clear cache"):
        if clear_cache():
            st.success("Cache cleared.")
    st.divider()
    st.caption("Qdrant diagnostics")
    st.write("Docs:", _collection_count(DOC_COLLECTION))
    st.write("Cache:", _collection_count(CACHE_COLLECTION))
    st.markdown("---")
    # Link to admin expander (open only for admin)
    if st.session_state.can_upload:
        st.write("You are admin â€” expand 'Admin Panel' below to manage uploads/settings.")

# Main layout: left chat, right admin (admin is hidden unless can_upload)
col1, col2 = st.columns([2, 1])

# Chat column (user-facing)
with col1:
    st.subheader("Chat")
    # display history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    question = st.chat_input("Ask a questionâ€¦")

    # Acquire runtime settings (from admin_settings if set)
    admin_cfg = st.session_state.admin_settings
    top_k = admin_cfg.get("top_k", TOP_K_DEFAULT)
    min_score = admin_cfg.get("min_score", MIN_SCORE_DEFAULT)
    context_char_cap = admin_cfg.get("context_char_cap", CONTEXT_CHAR_CAP_DEFAULT)
    num_predict = admin_cfg.get("num_predict", NUM_PREDICT_DEFAULT)
    model_name = admin_cfg.get("model_name", DEFAULT_MODEL)

    # Use UI bypass checkbox for this run
    bypass_cache = bool(bypass_cache_ui)

    if question:
        st.session_state.messages.append({"role":"user","content":question})
        with st.chat_message("user"):
            st.markdown(question)

        answered = False
        if not bypass_cache:
            exact_hit = exact_cache_get(question)
            if exact_hit:
                with st.chat_message("assistant"):
                    st.markdown(exact_hit + "\n\nâ€” _(exact cached)_")
                st.session_state.messages.append({"role":"assistant","content":exact_hit})
                answered = True
            else:
                sem_hit = semantic_cache_get(question, top_k=1, min_score=max(min_score, SEMANTIC_CACHE_MIN_SCORE))
                if sem_hit:
                    resp_text, sc = sem_hit
                    with st.chat_message("assistant"):
                        st.markdown(resp_text + f"\n\nâ€” _(semantic cached, score={sc:.3f})_")
                    st.session_state.messages.append({"role":"assistant","content":resp_text})
                    exact_cache_set(question, resp_text)
                    answered = True

        if not answered:
            # retrieve
            results = qdrant_search(question, top_k=top_k)
            if results and results.get("documents"):
                docs_list = results["documents"][0]
                metas_list = results["metadatas"][0]
                scores_list = results["scores"][0] if results.get("scores") else [0.0]*len(docs_list)
                # filter by min_score
                filtered_docs, filtered_metas, filtered_scores = [], [], []
                for d,m,sc in zip(docs_list, metas_list, scores_list):
                    if float(sc) >= float(min_score):
                        filtered_docs.append(d); filtered_metas.append(m); filtered_scores.append(sc)
                if filtered_docs:
                    results["documents"] = [filtered_docs]; results["metadatas"] = [filtered_metas]; results["scores"]=[filtered_scores]
                # if filter removed all, keep unfiltered to avoid dead-end (optional)
            # check again
            if (not results) or (not results.get("documents")) or (len(results["documents"][0])==0):
                with st.chat_message("assistant"):
                    st.warning("No relevant content found in your indexed documents. Try uploading sources or lowering Minimum similarity (admin).")
                    st.session_state.messages.append({"role":"assistant","content":"No relevant content found in your indexed documents."})
            else:
                # build context
                def build_context_from_results(results: Dict[str,List], max_chars: int):
                    context_parts=[]; shown_items=[]; used=0
                    docs_list=results.get("documents",[[]])[0]
                    metas_list=results.get("metadatas",[[]])[0]
                    scores_list=results.get("scores",[[]])[0] if results.get("scores") else [0.0]*len(docs_list)
                    for doc,meta,score in zip(docs_list,metas_list,scores_list):
                        src = meta.get("source_file","unknown")
                        idx = meta.get("chunk_index",-1)
                        tag = f"[{src}#{idx}]"
                        snippet = (doc or "").strip()
                        if not snippet:
                            continue
                        piece = f"{tag} {snippet}\n"
                        if used + len(piece) > max_chars:
                            break
                        context_parts.append(piece); used += len(piece)
                        shown_items.append((src, idx, score, snippet[:300]+("â€¦" if len(snippet)>300 else "")))
                    return "\n".join(context_parts), shown_items

                context_text, shown_items = build_context_from_results(results, context_char_cap)
                prompt = build_prompt(context_text, question)

                with st.chat_message("assistant"):
                    stream_area = st.empty()
                    answer_accum = ""
                    options = {"num_ctx":2048, "temperature":0, "num_predict":int(num_predict)}
                    for chunk in stream_ollama_answer(prompt, model_name=model_name, options=options):
                        answer_accum += chunk
                        stream_area.markdown(answer_accum + "â–Œ")
                    stream_area.markdown(answer_accum)
                    final_text = answer_accum if answer_accum else "I couldnâ€™t generate an answer."
                    st.session_state.messages.append({"role":"assistant","content":final_text})
                    exact_cache_set(question, final_text)
                    semantic_cache_set(question, final_text)

# Admin column (only visible if admin)
with col2:
    st.subheader("Admin Panel")
    if not st.session_state.can_upload:
        st.info("Admin options are available only to upload-enabled users.")
    else:
        # Upload
        # Upload
        uploaded_files = st.file_uploader(
        "Upload PDF / DOCX / TXT / MD (Admin)",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True
)
if uploaded_files:
    for uf in uploaded_files:
        # Display per-file controls
        st.write(f"ðŸ“„ {uf.name}")
        col_a, col_b = st.columns([1, 1])

        with col_a:
            if st.button(f"Index {uf.name}", key=f"index_{uf.name}"):
                with st.spinner("Indexingâ€¦"):
                    try:
                        ok, _ = ingest_uploaded_file(uf)
                        if ok:
                            st.success(f"Indexed {uf.name}")
                        else:
                            st.info(f"Skipped {uf.name} (already ingested)")
                    except Exception as e:
                        st.error(f"Failed {uf.name}: {e}")

        with col_b:
            if st.button(f"Force re-index {uf.name}", key=f"reindex_{uf.name}"):
                with st.spinner("Re-indexingâ€¦"):
                    try:
                        ok, _ = force_reindex_file(uf)
                        if ok:
                            st.success(f"Re-indexed {uf.name}")
                    except Exception as e:
                        st.error(f"Failed re-index {uf.name}: {e}")

# =============================
# Show already indexed files
# =============================

def list_indexed_files():
    try:
        scroll_res = qclient.scroll(
            collection_name=DOC_COLLECTION,
            limit=100,
            with_payload=True
        )
        points = scroll_res[0]
        files = {}
        for p in points:
            fname = p.payload.get("source_file", "unknown")
            sha = p.payload.get("sha256", "")
            size = len(p.payload.get("text", ""))
            if fname not in files:
                files[fname] = {"sha256": sha, "chunks": 0, "total_size": 0}
            files[fname]["chunks"] += 1
            files[fname]["total_size"] += size
        return files
    except Exception as e:
        st.warning(f"Could not fetch indexed docs: {e}")
        return {}

with st.expander("ðŸ“‚ Indexed Documents in Qdrant", expanded=False):
    files = list_indexed_files()
    if not files:
        st.info("No documents indexed yet.")
    else:
        for fname, info in files.items():
            st.write(
                f"**{fname}** â€“ {info['chunks']} chunks, {info['total_size']} chars, sha256: `{info['sha256'][:8]}â€¦`"
            )


        st.markdown("---")
        st.markdown("**Advanced settings**")
        # admin settings persisted in session_state
        s = st.session_state.admin_settings
        s["model_name"] = st.selectbox("Ollama model", options=[DEFAULT_MODEL, "llama3", "mistral"], index=0)
        s["top_k"] = st.slider("Top-K chunks", min_value=1, max_value=12, value=s.get("top_k", TOP_K_DEFAULT))
        s["min_score"] = st.slider("Minimum similarity (cosine)", 0.0, 1.0, value=s.get("min_score", MIN_SCORE_DEFAULT), step=0.01)
        s["context_char_cap"] = st.number_input("Context character cap", min_value=1000, max_value=20000, value=s.get("context_char_cap", CONTEXT_CHAR_CAP_DEFAULT), step=500)
        s["num_predict"] = st.slider("Max tokens to generate (num_predict)", min_value=64, max_value=2048, value=s.get("num_predict", NUM_PREDICT_DEFAULT), step=32)
        st.success("Admin settings saved in session (persist until restart)")

        st.markdown("---")
        st.markdown("**Diagnostics**")
        st.write("Docs:", _collection_count(DOC_COLLECTION))
        st.write("Cache:", _collection_count(CACHE_COLLECTION))
        if st.button("Recreate collections to match embed dims"):
            ensure_collections(); st.success("Collections recreated/validated.")

