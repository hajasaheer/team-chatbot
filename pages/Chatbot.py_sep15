import streamlit as st
from utils.qa import (
    exact_cache_get, semantic_cache_get, exact_cache_set,
    semantic_cache_set, qdrant_search, build_prompt, stream_ollama_answer
)


client_id = st.selectbox("Select Client", ["ClientA", "ClientB", "ClientC"])
success, fname = ingest_uploaded_file(uploaded_file, client_id=client_id)

st.title("ğŸ’¬ Chatbot")

query = st.text_input("Ask a question about your documents")

if query:
    # 1ï¸âƒ£ Try exact cache first
    cached = exact_cache_get(query)

    # 2ï¸âƒ£ If not found, try semantic cache
    if not cached:
        sem_cache = semantic_cache_get(query)
        if sem_cache:
            cached, score = sem_cache
            st.info(f"From semantic cache (score={score:.2f})")

    # 3ï¸âƒ£ If cache hit â†’ display
    if cached:
        st.write(cached)

    else:
        # ğŸ” Retrieve context from Qdrant
        results = qdrant_search(query)
        if not results:
            st.warning("No relevant documents found.")
        else:
            docs = [r.payload.get("text", "") for r in results if r.payload]
            context = "\n\n".join(docs)

            # ğŸ“ Build prompt (fixed arg order: query, then context)
            prompt = build_prompt(query, context)

            # ğŸ¤– Stream response
            st.write("ğŸ¤– Answer:")
            response_text = ""
            resp_container = st.empty()
            for chunk in stream_ollama_answer(prompt, "llama3"):
                response_text += chunk
                resp_container.markdown(response_text)

            # ğŸ’¾ Cache result in both exact + semantic cache
            exact_cache_set(query, response_text)
            semantic_cache_set(query, response_text)
