import os
import io
import json
import hashlib
import tempfile
from datetime import datetime

import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaLLM

# -----------------------------
# Simple login with two roles
# -----------------------------
CHATBOT_PASSWORD = os.getenv("CHATBOT_PASSWORD", "chatbot123")   # Chat access only
UPLOAD_PASSWORD = os.getenv("UPLOAD_PASSWORD", "upload123")      # Chat + Upload access

if "authenticated" not in st.session_state:
    st.session_state["authenticated"] = False
if "can_upload" not in st.session_state:
    st.session_state["can_upload"] = False

if not st.session_state["authenticated"]:
    with st.form("login", clear_on_submit=True):
        pw = st.text_input("Password", type="password")
        submit = st.form_submit_button("Login")
        if submit:
            if pw == CHATBOT_PASSWORD:
                st.session_state["authenticated"] = True
                st.session_state["can_upload"] = False
                st.success("âœ… Logged in (chat only)")
            elif pw == UPLOAD_PASSWORD:
                st.session_state["authenticated"] = True
                st.session_state["can_upload"] = True
                st.success("âœ… Logged in (chat + upload)")
            else:
                st.error("âŒ Wrong password")
    st.stop()

# -----------------------------
# Settings
# -----------------------------
INDEX_DIR = "vector_store"               # FAISS index directory (persistent)
UPLOADS_DIR = "uploads"                  # Where we keep original files
MANIFEST_PATH = os.path.join(INDEX_DIR, "manifest.json")  # Track what's ingested

DEFAULT_MODEL = "mistral"                # Fast on CPU (Ollama)
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # fast+small CPU embedding
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 120
TOP_K = 4

os.makedirs(UPLOADS_DIR, exist_ok=True)

# -----------------------------
# Helpers
# -----------------------------
def file_sha256(data: bytes) -> str:
    h = hashlib.sha256()
    h.update(data)
    return h.hexdigest()

def load_manifest():
    if not os.path.exists(MANIFEST_PATH):
        return {"files": {}}
    with open(MANIFEST_PATH, "r") as f:
        return json.load(f)

def save_manifest(mani):
    os.makedirs(INDEX_DIR, exist_ok=True)
    with open(MANIFEST_PATH, "w") as f:
        json.dump(mani, f, indent=2)

@st.cache_resource(show_spinner=False)
def get_embeddings():
    return HuggingFaceEmbeddings(model_name=EMBED_MODEL)

@st.cache_resource(show_spinner=False)
def load_vector_store_cached():
    if os.path.exists(INDEX_DIR):
        try:
            return FAISS.load_local(INDEX_DIR, get_embeddings(), allow_dangerous_deserialization=True)
        except Exception:
            return None
    return None

def persist_vector_store(vs: FAISS):
    vs.save_local(INDEX_DIR)

def load_docs_from_path(path: str):
    lower = path.lower()
    if lower.endswith(".pdf"):
        loader = PyPDFLoader(path)
    elif lower.endswith(".docx"):
        loader = Docx2txtLoader(path)
    elif lower.endswith(".txt") or lower.endswith(".md"):
        loader = TextLoader(path, encoding="utf-8")
    else:
        raise ValueError("Unsupported file type.")
    return loader.load()

def split_docs(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    return splitter.split_documents(docs)

def ensure_vector_store():
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = load_vector_store_cached()
    return st.session_state.vector_store

def update_vector_store(new_docs):
    vs = ensure_vector_store()
    if vs is None:
        vs = FAISS.from_documents(new_docs, get_embeddings())
        st.session_state.vector_store = vs
    else:
        vs.add_documents(new_docs)
    persist_vector_store(vs)

def ingest_uploaded_file(uploaded_file):
    data = uploaded_file.read()
    digest = file_sha256(data)
    ext = os.path.splitext(uploaded_file.name)[1].lower()
    safe_name = f"{digest}{ext}"
    disk_path = os.path.join(UPLOADS_DIR, safe_name)

    mani = load_manifest()
    if digest in mani["files"]:
        return False, mani["files"][digest]["original_name"]

    with open(disk_path, "wb") as f:
        f.write(data)

    docs = load_docs_from_path(disk_path)
    for d in docs:
        d.metadata = d.metadata or {}
        d.metadata.update({
            "source_file": uploaded_file.name,
            "sha256": digest,
        })
    chunks = split_docs(docs)
    update_vector_store(chunks)

    mani["files"][digest] = {
        "original_name": uploaded_file.name,
        "stored_path": disk_path,
        "ingested_at": datetime.utcnow().isoformat() + "Z",
        "num_chunks": len(chunks)
    }
    save_manifest(mani)
    return True, uploaded_file.name

def stream_ollama_answer(prompt, model_name, options=None):
    llm = OllamaLLM(model=model_name, options=options or {})
    text = llm.invoke(prompt)
    for i in range(0, len(text), 20):
        yield text[i:i+20]

def build_prompt(context: str, question: str) -> str:
    return (
        "You are a concise assistant. Use ONLY the provided context. "
        "If the answer cannot be found in context, say you don't know.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {question}\n"
        "Answer with brief, direct sentences and cite the source file names if relevant."
    )

# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="ðŸ“š Team Knowledge Chatbot", layout="wide")
st.title("ðŸ“š Team Knowledge Chatbot")

# Sidebar controls
with st.sidebar:
    st.subheader("Settings")
    model = st.selectbox(
        "Ollama model",
        options=["mistral", "llama3.1:8b-instruct", "phi3", "llama3"],
        index=0,
        help="Smaller models answer faster on CPU. 'mistral' is a good default."
    )
    top_k = st.slider("Top-K chunks", min_value=2, max_value=8, value=TOP_K, step=1)
    show_sources = st.checkbox("Show retrieved sources", value=True)
    st.markdown("---")
    st.markdown("**Ingested files:**")
    mani = load_manifest()
    if mani["files"]:
        for info in mani["files"].values():
            st.write(f"- {info['original_name']}  _(chunks: {info['num_chunks']})_")
    else:
        st.caption("No files ingested yet.")

# Init chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Persisted vector store
ensure_vector_store()

# -----------------------------
# File Upload (restricted)
# -----------------------------
if st.session_state.get("can_upload", False):
    uploaded_files = st.file_uploader(
        "Upload PDF / DOCX / TXT (you can upload multiple files)",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True
    )

    if uploaded_files:
        with st.spinner("Indexingâ€¦"):
            added = 0
            skipped = 0
            for uf in uploaded_files:
                try:
                    ok, _ = ingest_uploaded_file(uf)
                    if ok: added += 1
                    else: skipped += 1
                except Exception as e:
                    st.error(f"Failed to ingest {uf.name}: {e}")
            if added:
                st.success(f"Indexed {added} file(s).")
            if skipped:
                st.info(f"Skipped {skipped} already-ingested file(s).")
else:
    st.info("ðŸ“‚ File upload restricted. You only have chat access.")

# -----------------------------
# Chat Interface
# -----------------------------
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

question = st.chat_input("Ask a question about your documentsâ€¦")

if question:
    if st.session_state.vector_store is None:
        st.warning("Please upload and index at least one document first.")
    else:
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        retriever = st.session_state.vector_store.as_retriever(search_kwargs={"k": top_k})
        results = retriever.get_relevant_documents(question)

        context_parts = []
        shown_sources = []
        for d in results:
            src = d.metadata.get("source_file", "unknown")
            snippet = d.page_content.strip().replace("\n", " ")
            context_parts.append(f"[{src}] {snippet}")
            if show_sources:
                shown_sources.append(src)
        context_text = "\n\n".join(context_parts[:top_k])

        prompt = build_prompt(context_text, question)
        with st.chat_message("assistant"):
            stream_area = st.empty()
            answer_accum = ""
            options = {"num_ctx": 2048, "temperature": 0, "num_thread": 4}
            for chunk in stream_ollama_answer(prompt, model_name=model, options=options):
                answer_accum += chunk
                stream_area.markdown(answer_accum)
            if show_sources and shown_sources:
                unique_src = sorted(set(shown_sources))
                stream_area.markdown(answer_accum + "\n\nâ€” _Sources:_ " + ", ".join(unique_src))

        final_text = answer_accum if answer_accum else "I couldn't generate an answer."
        if show_sources and shown_sources:
            final_text += "\n\nâ€” _Sources:_ " + ", ".join(sorted(set(shown_sources)))
        st.session_state.messages.append({"role": "assistant", "content": final_text})
